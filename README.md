
LiveTranslate

A real-time speech-to-text + gesture recognition web app that combines the Web Speech API, webcam-based gesture detection, and a beautifully polished glassmorphic UI.

LiveTranslate allows users to speak into their microphone, see text updates live, train custom gestures, and run predictions directly in the browser â€” no backend required.

â¸»

ğŸš€ Features

ğŸ™ï¸ Live Speech Recognition
	â€¢	Uses the Web Speech API for continuous speech-to-text.
	â€¢	Real-time transcription displayed in a dedicated output panel.
	â€¢	Interactive audio waveform generated via the Web Audio API.
	â€¢	Start/stop listening with a single click.

âœ‹ Gesture Recognition (ASL or Custom Gestures)
	â€¢	Live webcam feed with adjustable canvas overlay.
	â€¢	Users can:
	â€¢	Enter a custom label
	â€¢	Capture gesture samples
	â€¢	Train a custom model directly in the browser
	â€¢	Real-time prediction button to classify the current gesture.

ğŸ“¸ Camera System
	â€¢	Live stream using getUserMedia().
	â€¢	Status indicator (ACTIVE/ERROR).
	â€¢	Hidden canvas used for gesture processing.

ğŸ–¥ï¸ Modern UI
	â€¢	Fully glassmorphic interface.
	â€¢	Smooth gradients, soft shadows, and responsive layout.
	â€¢	Animated buttons with shimmer effect.
	â€¢	Hover + focus transitions for professional feel.

â¸»

ğŸ› ï¸ Tech Stack

-Area	Technology
-Speech Recognition	Web Speech API
-Audio Visualization	Web Audio API (AnalyserNode, Canvas)
-Gesture System	Webcam + Canvas capture
-Model Training	Browser-based JS (TensorFlow.js compatible)
-Frontend	Vanilla JavaScript, HTML5, CSS3
-Styling	Custom gradients, glassmorphism, responsive design


â¸»

ğŸ“¦ Folder Structure (Recommended)

/LiveTranslate
â”‚â”€â”€ index.html
â”‚â”€â”€ style.css         # optional if separating styles
â”‚â”€â”€ script.js         # optional if separating scripts
â”‚â”€â”€ assets/
â”‚     â””â”€â”€ icons/
â”‚     â””â”€â”€ models/
â””â”€â”€ README.md


â¸»

â–¶ï¸ How to Run
1.	Clone the repository:

        git clone https://github.com/yourusername/LiveTranslate.git

2.	Open the folder:

        cd LiveTranslate

3.	Simply open index.html in your browser.

No server needed â€” everything works client-side.

For gesture training to access your camera/mic, Chrome may require you to run from a local server.
Use this if necessary:

    python3 -m http.server

Then visit:
http://localhost:8000

â¸»

âœ¨ Core Functions Explained

ğŸ”Š startListening()
	â€¢	Starts microphone recognition
	â€¢	Begins drawing audio waveform
	â€¢	Updates speech output live

ğŸ¥ Webcam + Gesture Capture
	â€¢	Streams webcam into <video>
	â€¢	Canvas grabs frames for model training
	â€¢	Captured samples saved with a label

ğŸ¤– startTraining()
	â€¢	Trains a small browser ML model
	â€¢	Learns your custom gestures
	â€¢	Updates UI after training

ğŸ”® showPrediction()
	â€¢	Runs inference on live video
	â€¢	Displays predicted gesture label

â¸»

ğŸ§ª Future Improvements
	â€¢	TensorFlow.js integration for better accuracy
	â€¢	Pretrained ASL model support
	â€¢	Noise-robust speech recognition
	â€¢	UI dark/light themes
	â€¢	Save & load custom models

â¸»

ğŸ“„ License

MIT License â€” free to modify and distribute.

â¸»

ğŸ’¬ Contributions

PRs are welcome!
You can improve:
	â€¢	Model accuracy
	â€¢	UI animations
	â€¢	Documentation
	â€¢	Cross-browser support

â¸»

